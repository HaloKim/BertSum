{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BertSum Test",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVuplEqWg2Om"
      },
      "source": [
        "# Load pyrouge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JOBFcBn_OLz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e60ce27-6faf-415d-86aa-600052b66e04"
      },
      "source": [
        "!pip install pyrouge --upgrade\n",
        "!pip install https://github.com/bheinzerling/pyrouge/archive/master.zip\n",
        "!pip install pyrouge\n",
        "!pip show pyrouge\n",
        "!git clone https://github.com/andersjo/pyrouge.git\n",
        "from pyrouge import Rouge155\n",
        "!pyrouge_set_rouge_path 'pyrouge/tools/ROUGE-1.5.5'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyrouge\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/85/e522dd6b36880ca19dcf7f262b22365748f56edc6f455e7b6a37d0382c32/pyrouge-0.1.3.tar.gz (60kB)\n",
            "\r\u001b[K     |█████▍                          | 10kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 20kB 21.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 30kB 25.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 40kB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 51kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 5.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyrouge\n",
            "  Building wheel for pyrouge (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyrouge: filename=pyrouge-0.1.3-cp37-none-any.whl size=191621 sha256=33cb8cfdfe38615c88335e1df807bb6eb845af4ad0ae34179d2e84b508fca190\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/d3/0c/e5b04e15b6b87c42e980de3931d2686e14d36e045058983599\n",
            "Successfully built pyrouge\n",
            "Installing collected packages: pyrouge\n",
            "Successfully installed pyrouge-0.1.3\n",
            "Collecting https://github.com/bheinzerling/pyrouge/archive/master.zip\n",
            "\u001b[?25l  Downloading https://github.com/bheinzerling/pyrouge/archive/master.zip\n",
            "\u001b[K     / 696kB 14.1MB/s\n",
            "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): pyrouge==0.1.3 from https://github.com/bheinzerling/pyrouge/archive/master.zip in /usr/local/lib/python3.7/dist-packages\n",
            "Building wheels for collected packages: pyrouge\n",
            "  Building wheel for pyrouge (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyrouge: filename=pyrouge-0.1.3-cp37-none-any.whl size=191921 sha256=d90c12119a018d252d63200747f060d1d91fba496b8fd1170dfa6b63da759e6a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wam9f0fl/wheels/70/02/b4/a23b5feb5980a5eb940441cb04ec1e17d5f18344138efbecf8\n",
            "Successfully built pyrouge\n",
            "Requirement already satisfied: pyrouge in /usr/local/lib/python3.7/dist-packages (0.1.3)\n",
            "Name: pyrouge\n",
            "Version: 0.1.3\n",
            "Summary: A Python wrapper for the ROUGE summarization evaluation package.\n",
            "Home-page: https://github.com/noutenki/pyrouge\n",
            "Author: Benjamin Heinzerling, Anders Johannsen\n",
            "Author-email: benjamin.heinzerling@h-its.org\n",
            "License: LICENSE.txt\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: \n",
            "Required-by: \n",
            "Cloning into 'pyrouge'...\n",
            "remote: Enumerating objects: 393, done.\u001b[K\n",
            "remote: Total 393 (delta 0), reused 0 (delta 0), pack-reused 393\u001b[K\n",
            "Receiving objects: 100% (393/393), 298.74 KiB | 3.69 MiB/s, done.\n",
            "Resolving deltas: 100% (109/109), done.\n",
            "2021-06-10 03:27:11,793 [MainThread  ] [INFO ]  Set ROUGE home directory to pyrouge/tools/ROUGE-1.5.5.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BxQRUc5KeGG",
        "outputId": "834e498c-019f-43ea-e5fe-fb8ea08018cb"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install tensorboardX\n",
        "!pip install easydict"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 15.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 40.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 53.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: huggingface-hub, tokenizers, sacremoses, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 14.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (57.0.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.2\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.7/dist-packages (1.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amSkThJD-Bxx",
        "outputId": "83004791-9207-4024-cdf1-eb495aa1db2d"
      },
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/HaloKim/KorBertSum.git\n",
        "%cd /content/KorBertSum"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'KorBertSum'...\n",
            "remote: Enumerating objects: 11117, done.\u001b[K\n",
            "remote: Counting objects: 100% (10815/10815), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7591/7591), done.\u001b[K\n",
            "remote: Total 11117 (delta 3243), reused 10768 (delta 3214), pack-reused 302\u001b[K\n",
            "Receiving objects: 100% (11117/11117), 18.77 MiB | 14.11 MiB/s, done.\n",
            "Resolving deltas: 100% (3418/3418), done.\n",
            "/content/KorBertSum\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-yEOMhAAPWs"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkUneUqUTtZ9"
      },
      "source": [
        "import os\n",
        "\n",
        "os.chdir('/content/KorBertSum/src')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ggKByDgUfp8"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from models import data_loader, model_builder\n",
        "from models.model_builder import Summarizer\n",
        "from others.logging import logger, init_logger\n",
        "from models.data_loader import load_dataset\n",
        "from transformers import BertConfig\n",
        "from tensorboardX import SummaryWriter\n",
        "from models.reporter import ReportMgr\n",
        "from models.stats import Statistics\n",
        "import easydict\n",
        "from transformers import BertTokenizer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9qjyO4fND_h"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeRgtmVDLgRy"
      },
      "source": [
        "def _tally_parameters(model):\n",
        "    n_params = sum([p.nelement() for p in model.parameters()])\n",
        "    return n_params\n",
        "\n",
        "def build_trainer(args, device_id, model,\n",
        "                  optim):\n",
        "    \"\"\"\n",
        "    Simplify `Trainer` creation based on user `opt`s*\n",
        "    Args:\n",
        "        opt (:obj:`Namespace`): user options (usually from argument parsing)\n",
        "        model (:obj:`onmt.models.NMTModel`): the model to train\n",
        "        fields (dict): dict of fields\n",
        "        optim (:obj:`onmt.utils.Optimizer`): optimizer used during training\n",
        "        data_type (str): string describing the type of data\n",
        "            e.g. \"text\", \"img\", \"audio\"\n",
        "        model_saver(:obj:`onmt.models.ModelSaverBase`): the utility object\n",
        "            used to save the model\n",
        "    \"\"\"\n",
        "    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
        "\n",
        "\n",
        "    grad_accum_count = args.accum_count\n",
        "    n_gpu = args.world_size\n",
        "\n",
        "    if device_id >= 0:\n",
        "        gpu_rank = int(args.gpu_ranks[device_id])\n",
        "    else:\n",
        "        gpu_rank = 0\n",
        "        n_gpu = 0\n",
        "\n",
        "    print('gpu_rank %d' % gpu_rank)\n",
        "\n",
        "    tensorboard_log_dir = args.model_path\n",
        "\n",
        "    writer = SummaryWriter(tensorboard_log_dir, comment=\"Unmt\")\n",
        "\n",
        "    report_manager = ReportMgr(args.report_every, start_time=-1, tensorboard_writer=writer)\n",
        "\n",
        "    trainer = Trainer(args, model, optim, grad_accum_count, n_gpu, gpu_rank, report_manager)\n",
        "\n",
        "    # print(tr)\n",
        "    if (model):\n",
        "        n_params = _tally_parameters(model)\n",
        "        logger.info('* number of parameters: %d' % n_params)\n",
        "\n",
        "    return trainer\n",
        "\n",
        "\n",
        "class Trainer(object):\n",
        "    \"\"\"\n",
        "    Class that controls the training process.\n",
        "\n",
        "    Args:\n",
        "            model(:py:class:`onmt.models.model.NMTModel`): translation model\n",
        "                to train\n",
        "            train_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n",
        "               training loss computation\n",
        "            valid_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n",
        "               training loss computation\n",
        "            optim(:obj:`onmt.utils.optimizers.Optimizer`):\n",
        "               the optimizer responsible for update\n",
        "            trunc_size(int): length of truncated back propagation through time\n",
        "            shard_size(int): compute loss in shards of this size for efficiency\n",
        "            data_type(string): type of the source input: [text|img|audio]\n",
        "            norm_method(string): normalization methods: [sents|tokens]\n",
        "            grad_accum_count(int): accumulate gradients this many times.\n",
        "            report_manager(:obj:`onmt.utils.ReportMgrBase`):\n",
        "                the object that creates reports, or None\n",
        "            model_saver(:obj:`onmt.models.ModelSaverBase`): the saver is\n",
        "                used to save a checkpoint.\n",
        "                Thus nothing will be saved if this parameter is None\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,  args, model,  optim,\n",
        "                  grad_accum_count=1, n_gpu=1, gpu_rank=1,\n",
        "                  report_manager=None):\n",
        "        # Basic attributes.\n",
        "        self.args = args\n",
        "        self.save_checkpoint_steps = args.save_checkpoint_steps\n",
        "        self.model = model\n",
        "        self.optim = optim\n",
        "        self.grad_accum_count = grad_accum_count\n",
        "        self.n_gpu = n_gpu\n",
        "        self.gpu_rank = gpu_rank\n",
        "        self.report_manager = report_manager\n",
        "\n",
        "        self.loss = torch.nn.BCELoss(reduction='none')\n",
        "        assert grad_accum_count > 0\n",
        "        # Set model in training mode.\n",
        "        if (model):\n",
        "            self.model.train()\n",
        "\n",
        "    def summ(self, test_iter, step, cal_lead=False, cal_oracle=False):\n",
        "      \"\"\" Validate model.\n",
        "          valid_iter: validate data iterator\n",
        "      Returns:\n",
        "          :obj:`nmt.Statistics`: validation loss statistics\n",
        "      \"\"\"\n",
        "      # Set model in validating mode.\n",
        "      def _get_ngrams(n, text):\n",
        "          ngram_set = set()\n",
        "          text_length = len(text)\n",
        "          max_index_ngram_start = text_length - n\n",
        "          for i in range(max_index_ngram_start + 1):\n",
        "              ngram_set.add(tuple(text[i:i + n]))\n",
        "          return ngram_set\n",
        "\n",
        "      def _block_tri(c, p):\n",
        "          tri_c = _get_ngrams(3, c.split())\n",
        "          for s in p:\n",
        "              tri_s = _get_ngrams(3, s.split())\n",
        "              if len(tri_c.intersection(tri_s))>0:\n",
        "                  return True\n",
        "          return False\n",
        "\n",
        "      if (not cal_lead and not cal_oracle):\n",
        "          self.model.eval()\n",
        "      stats = Statistics()\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for batch in test_iter:\n",
        "              src = batch.src\n",
        "              labels = batch.labels\n",
        "              segs = batch.segs\n",
        "              clss = batch.clss\n",
        "              mask = batch.mask\n",
        "              mask_cls = batch.mask_cls\n",
        "\n",
        "              if (cal_lead):\n",
        "                  selected_ids = [list(range(batch.clss.size(1)))] * batch.batch_size\n",
        "              elif (cal_oracle):\n",
        "                  selected_ids = [[j for j in range(batch.clss.size(1)) if labels[i][j] == 1] for i in\n",
        "                                  range(batch.batch_size)]\n",
        "              else:\n",
        "                  sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
        "                  sent_scores = sent_scores + mask.float()\n",
        "                  sent_scores = sent_scores.cpu().data.numpy()\n",
        "                  selected_ids = np.argsort(-sent_scores, 1)\n",
        "      return selected_ids\n",
        "\n",
        "\n",
        "\n",
        "    def _gradient_accumulation(self, true_batchs, normalization, total_stats,\n",
        "                               report_stats):\n",
        "        if self.grad_accum_count > 1:\n",
        "            self.model.zero_grad()\n",
        "\n",
        "        for batch in true_batchs:\n",
        "            if self.grad_accum_count == 1:\n",
        "                self.model.zero_grad()\n",
        "\n",
        "            src = batch.src\n",
        "            labels = batch.labels\n",
        "            segs = batch.segs\n",
        "            clss = batch.clss\n",
        "            mask = batch.mask\n",
        "            mask_cls = batch.mask_cls\n",
        "\n",
        "            sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
        "\n",
        "            loss = self.loss(sent_scores, labels.float())\n",
        "            loss = (loss*mask.float()).sum()\n",
        "            (loss/loss.numel()).backward()\n",
        "            # loss.div(float(normalization)).backward()\n",
        "\n",
        "            batch_stats = Statistics(float(loss.cpu().data.numpy()), normalization)\n",
        "\n",
        "\n",
        "            total_stats.update(batch_stats)\n",
        "            report_stats.update(batch_stats)\n",
        "\n",
        "            # 4. Update the parameters and statistics.\n",
        "            if self.grad_accum_count == 1:\n",
        "                # Multi GPU gradient gather\n",
        "                if self.n_gpu > 1:\n",
        "                    grads = [p.grad.data for p in self.model.parameters()\n",
        "                             if p.requires_grad\n",
        "                             and p.grad is not None]\n",
        "                    distributed.all_reduce_and_rescale_tensors(\n",
        "                        grads, float(1))\n",
        "                self.optim.step()\n",
        "\n",
        "        # in case of multi step gradient accumulation,\n",
        "        # update only after accum batches\n",
        "        if self.grad_accum_count > 1:\n",
        "            if self.n_gpu > 1:\n",
        "                grads = [p.grad.data for p in self.model.parameters()\n",
        "                         if p.requires_grad\n",
        "                         and p.grad is not None]\n",
        "                distributed.all_reduce_and_rescale_tensors(\n",
        "                    grads, float(1))\n",
        "            self.optim.step()\n",
        "\n",
        "    def _save(self, step):\n",
        "        real_model = self.model\n",
        "        # real_generator = (self.generator.module\n",
        "        #                   if isinstance(self.generator, torch.nn.DataParallel)\n",
        "        #                   else self.generator)\n",
        "\n",
        "        model_state_dict = real_model.state_dict()\n",
        "        # generator_state_dict = real_generator.state_dict()\n",
        "        checkpoint = {\n",
        "            'model': model_state_dict,\n",
        "            # 'generator': generator_state_dict,\n",
        "            'opt': self.args,\n",
        "            'optim': self.optim,\n",
        "        }\n",
        "        checkpoint_path = os.path.join(self.args.model_path, 'model_step_%d.pt' % step)\n",
        "        logger.info(\"Saving checkpoint %s\" % checkpoint_path)\n",
        "        # checkpoint_path = '%s_step_%d.pt' % (FLAGS.model_path, step)\n",
        "        if (not os.path.exists(checkpoint_path)):\n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "            return checkpoint, checkpoint_path\n",
        "\n",
        "    def _start_report_manager(self, start_time=None):\n",
        "        \"\"\"\n",
        "        Simple function to start report manager (if any)\n",
        "        \"\"\"\n",
        "        if self.report_manager is not None:\n",
        "            if start_time is None:\n",
        "                self.report_manager.start()\n",
        "            else:\n",
        "                self.report_manager.start_time = start_time\n",
        "\n",
        "    def _maybe_gather_stats(self, stat):\n",
        "        \"\"\"\n",
        "        Gather statistics in multi-processes cases\n",
        "\n",
        "        Args:\n",
        "            stat(:obj:onmt.utils.Statistics): a Statistics object to gather\n",
        "                or None (it returns None in this case)\n",
        "\n",
        "        Returns:\n",
        "            stat: the updated (or unchanged) stat object\n",
        "        \"\"\"\n",
        "        if stat is not None and self.n_gpu > 1:\n",
        "            return Statistics.all_gather_stats(stat)\n",
        "        return stat\n",
        "\n",
        "    def _maybe_report_training(self, step, num_steps, learning_rate,\n",
        "                               report_stats):\n",
        "        \"\"\"\n",
        "        Simple function to report training stats (if report_manager is set)\n",
        "        see `onmt.utils.ReportManagerBase.report_training` for doc\n",
        "        \"\"\"\n",
        "        if self.report_manager is not None:\n",
        "            return self.report_manager.report_training(\n",
        "                step, num_steps, learning_rate, report_stats,\n",
        "                multigpu=self.n_gpu > 1)\n",
        "\n",
        "    def _report_step(self, learning_rate, step, train_stats=None,\n",
        "                     valid_stats=None):\n",
        "        \"\"\"\n",
        "        Simple function to report stats (if report_manager is set)\n",
        "        see `onmt.utils.ReportManagerBase.report_step` for doc\n",
        "        \"\"\"\n",
        "        if self.report_manager is not None:\n",
        "            return self.report_manager.report_step(\n",
        "                learning_rate, step, train_stats=train_stats,\n",
        "                valid_stats=valid_stats)\n",
        "\n",
        "    def _maybe_save(self, step):\n",
        "        \"\"\"\n",
        "        Save the model if a model saver is set\n",
        "        \"\"\"\n",
        "        if self.model_saver is not None:\n",
        "            self.model_saver.maybe_save(step)\n",
        "\n",
        "class BertData():\n",
        "    def __init__(self):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)\n",
        "        self.sep_vid = self.tokenizer.vocab['[SEP]']\n",
        "        self.cls_vid = self.tokenizer.vocab['[CLS]']\n",
        "        self.pad_vid = self.tokenizer.vocab['[PAD]']\n",
        "\n",
        "    def preprocess(self, src):\n",
        "\n",
        "        if (len(src) == 0):\n",
        "            return None\n",
        "\n",
        "        original_src_txt = [' '.join(s) for s in src]\n",
        "        idxs = [i for i, s in enumerate(src) if (len(s) > 1)]\n",
        "\n",
        "        src = [src[i][:2000] for i in idxs]\n",
        "        src = src[:1000]\n",
        "\n",
        "        if (len(src) < 3):\n",
        "            return None\n",
        "        if (len(labels) == 0):\n",
        "            return None\n",
        "\n",
        "        src_txt = [' '.join(sent) for sent in src]\n",
        "        text = ' [SEP] [CLS] '.join(src_txt)\n",
        "        src_subtokens = self.tokenizer.tokenize(text)\n",
        "        src_subtokens = src_subtokens[:510]\n",
        "        src_subtokens = ['[CLS]'] + src_subtokens + ['[SEP]']\n",
        "\n",
        "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
        "        _segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
        "        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
        "        segments_ids = []\n",
        "        for i, s in enumerate(segs):\n",
        "            if (i % 2 == 0):\n",
        "                segments_ids += s * [0]\n",
        "            else:\n",
        "                segments_ids += s * [1]\n",
        "        cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]\n",
        "        labels = None\n",
        "        src_txt = [original_src_txt[i] for i in idxs]\n",
        "        tgt_txt = None\n",
        "        return src_subtoken_idxs, labels, segments_ids, cls_ids, src_txt, tgt_txt\n",
        "\n",
        "def _lazy_dataset_loader(pt_file):\n",
        "  yield  pt_file"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC_QmVS6NGvy"
      },
      "source": [
        "# Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tzZYPbLbRl7"
      },
      "source": [
        "args = easydict.EasyDict({\n",
        "    \"encoder\":'classifier',\n",
        "    \"mode\":'test',\n",
        "    \"bert_data_path\":'/content/drive/MyDrive/Colab_Notebooks/Study/BertSum-master/bert_data/korean',\n",
        "    \"model_path\":'./models/bert_classifier',\n",
        "    \"result_path\":'./results',\n",
        "    \"temp_dir\":'./temp',\n",
        "    \"batch_size\":1000,\n",
        "    \"use_interval\":True,\n",
        "    \"hidden_size\":128,\n",
        "    \"ff_size\":512,\n",
        "    \"heads\":4,\n",
        "    \"inter_layers\":2,\n",
        "    \"rnn_size\":512,\n",
        "    \"param_init\":0,\n",
        "    \"param_init_glorot\":True,\n",
        "    \"dropout\":0.1,\n",
        "    \"optim\":'adam',\n",
        "    \"lr\":2e-3,\n",
        "    \"report_every\":1,\n",
        "    \"save_checkpoint_steps\":5,\n",
        "    \"block_trigram\":True,\n",
        "    \"recall_eval\":False,\n",
        "    \n",
        "    \"accum_count\":1,\n",
        "    \"world_size\":1,\n",
        "    \"visible_gpus\":'-1',\n",
        "    \"gpu_ranks\":'0',\n",
        "    \"log_file\":'/content/drive/MyDrive/Colab_Notebooks/Study/BertSum-master/logs/log.log',\n",
        "    \"test_from\":'/content/drive/MyDrive/Colab_Notebooks/Study/BertSum-master/models/bert_classifier/model_step_1000.pt'\n",
        "})\n",
        "model_flags = ['hidden_size', 'ff_size', 'heads', 'inter_layers','encoder','ff_actv', 'use_interval','rnn_size']\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ1pvUQBNITZ"
      },
      "source": [
        "# Test code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYV7oXExF2je"
      },
      "source": [
        "def test(args, input_list, device_id, pt, step):\n",
        "  init_logger(args.log_file)\n",
        "  device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
        "  device_id = 0 if device == \"cuda\" else -1\n",
        "\n",
        "  cp = args.test_from\n",
        "  try:\n",
        "    step = int(cp.split('.')[-2].split('_')[-1])\n",
        "  except:\n",
        "    step = 0\n",
        "\n",
        "  device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
        "  if (pt != ''):\n",
        "      test_from = pt\n",
        "  else:\n",
        "      test_from = args.test_from\n",
        "  logger.info('Loading checkpoint from %s' % test_from)\n",
        "  checkpoint = torch.load(test_from, map_location=lambda storage, loc: storage)\n",
        "  opt = vars(checkpoint['opt'])\n",
        "  for k in opt.keys():\n",
        "      if (k in model_flags):\n",
        "        setattr(args, k, opt[k])\n",
        "\n",
        "  config = BertConfig.from_pretrained('bert-base-multilingual-cased')\n",
        "  model = Summarizer(args, device, load_pretrained_bert=False, bert_config = config)\n",
        "  model.load_cp(checkpoint)\n",
        "  model.eval()\n",
        "\n",
        "  test_iter = data_loader.Dataloader(args, _lazy_dataset_loader(input_list),\n",
        "                                args.batch_size, device,\n",
        "                                shuffle=False, is_test=True)\n",
        "  trainer = build_trainer(args, device_id, model, None)\n",
        "  result = trainer.summ(test_iter,step)\n",
        "  return result, input_list\n",
        "\n",
        "args.gpu_ranks = [int(i) for i in args.gpu_ranks.split(',')]\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.visible_gpus"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AywahUrUkPzB"
      },
      "source": [
        "def txt2input(text):\n",
        "  data = list(filter(None, text.split('\\n')))\n",
        "  bertdata = BertData()\n",
        "  txt_data = bertdata.preprocess(data)\n",
        "  data_dict = {\"src\":txt_data[0],\n",
        "               \"labels\":[0,1,2],\n",
        "               \"segs\":txt_data[2],\n",
        "               \"clss\":txt_data[3],\n",
        "               \"src_txt\":txt_data[4],\n",
        "               \"tgt_txt\":None}\n",
        "  input_data = []\n",
        "  input_data.append(data_dict)\n",
        "  return input_data"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXZk2tZvePTK"
      },
      "source": [
        "text = '''\n",
        "환자로 찾아왔던 공군 여성 장교를 성폭행하려다 미수에 그친 국군수도병원 의사가 재판에서 징역 3년 6개월을 선고받았다. 징역형을 선고받은 의사는 과거에 대통령 주치의를 역임했던 해당 분야의 권위자였다. \n",
        "\n",
        " \n",
        "지난달 대위로 전역한 전 공군장교A씨는 지난 2017년 국군병원 근무 중 육군 부사관에게 성추행을 당한 정신적 충격으로 당시 국군수도병원 신경과에 근무하던 70세 B씨에게 치료를 받았다.  \n",
        " \n",
        "3년 뒤인 지난해 국군수도병원에서 다시 만난 B씨는A씨에게 3년 전 일을 거론한 뒤 조언을 하고 싶다며 식사를 제안했다. 며칠 후 저녁을 함께한 뒤 B씨는A씨를 근처 자신의 집으로 끌고 들어가 성폭행을 시도했다.  \n",
        " \n",
        "간신히 집밖으로 달아난 A씨는 외상 후 스트레스 장애, 해리성 기억상실증, 마비 등 증상을 겪다 일주일 후 부대에 신고했다.  \n",
        " \n",
        "B씨는 강제 추행 장면이 담긴 아파트 CCTV에 찍힌 영상을 본 뒤 범행을 인정했다.  \n",
        " \n",
        "B씨는 지난해 12월 강제추행, 강간치상 등 혐의로 구속돼 재판에 넘겨졌고 결국 징역형을 선고받았다.  \n",
        " \n",
        "징역 10년을 구형했던 군 검찰은 1심에 불복해 항소할 방침이다.  \n",
        "\n",
        "[출처: 중앙일보] 여장교 성폭행 시도 70대 의사, 알고보니 대통령 주치의 출신\n",
        "'''"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IlXBocEMzS_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "211e75d1-a6e4-48c0-aa9c-8012dae69825"
      },
      "source": [
        "input_data = txt2input(text)\n",
        "sum_list = test(args, input_data, -1, '', None)\n",
        "sum_list[0][0]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-06-10 03:28:52,721 INFO] Loading checkpoint from /content/drive/MyDrive/Colab_Notebooks/Study/BertSum-master/models/bert_classifier/model_step_1000.pt\n",
            "[2021-06-10 03:29:50,660 INFO] * number of parameters: 177854209\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "gpu_rank 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 6, 2, 0, 5, 1, 3, 7])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uOvhjs8M1-s"
      },
      "source": [
        "# Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIhOEG0HlMpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ccf4482-9222-4fea-8708-782ceca87bc9"
      },
      "source": [
        "[list(filter(None, text.split('\\n')))[i] for i in sum_list[0][0][:3]]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['3년 뒤인 지난해 국군수도병원에서 다시 만난 B씨는A씨에게 3년 전 일을 거론한 뒤 조언을 하고 싶다며 식사를 제안했다. 며칠 후 저녁을 함께한 뒤 B씨는A씨를 근처 자신의 집으로 끌고 들어가 성폭행을 시도했다.  ',\n",
              " '간신히 집밖으로 달아난 A씨는 외상 후 스트레스 장애, 해리성 기억상실증, 마비 등 증상을 겪다 일주일 후 부대에 신고했다.  ',\n",
              " '지난달 대위로 전역한 전 공군장교A씨는 지난 2017년 국군병원 근무 중 육군 부사관에게 성추행을 당한 정신적 충격으로 당시 국군수도병원 신경과에 근무하던 70세 B씨에게 치료를 받았다.  ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSVtNDS2Jiaa"
      },
      "source": [
        " "
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}